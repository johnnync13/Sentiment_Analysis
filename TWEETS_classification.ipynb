{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TWEETS_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yxfrD0KYyS0x",
        "F6oFAPQCzmkj",
        "DH6PW-jZz9zP",
        "4-_TD1-N0Fnw"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itCXNg8jxLOh"
      },
      "source": [
        "# Fase 1: Importar las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq0PRLJ5oW29"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install tf-models-nightly\n",
        "!pip install tf-nightly\n",
        "!pip install python-binance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Yb2-6OwfxI"
      },
      "source": [
        "# Fase 2: CREAR ORDER DE COMPRA-VENTA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKWC6q-1xcnp"
      },
      "source": [
        "API_KEY = \"\"\n",
        "API_SECRET= \"\"\n",
        "TRADE_QUANTITY = 100\n",
        "\n",
        "from binance.client import Client\n",
        "from binance.enums import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXI3cH4zwef8"
      },
      "source": [
        "\n",
        "client = Client(API_KEY, API_SECRET, tld='us')\n",
        "\n",
        "def order(side, quantity, symbol,order_type=ORDER_TYPE_MARKET):\n",
        "    try:\n",
        "        print(\"sending order\")\n",
        "        order = client.create_order(symbol=symbol, side=side, type=order_type, quantity=quantity)\n",
        "        print(order)\n",
        "    except Exception as e:\n",
        "        print(\"an exception occured - {}\".format(e))\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aPiCzUZoxXM"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0Ku4PaIph_I"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Anq7Mch78ZL"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1PXcNy5Alhc"
      },
      "source": [
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp import optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTRZfD2m-m25"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB3qvun2x4jR"
      },
      "source": [
        "# Fase 2: Pre Procesado de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQO1hx4x7z5"
      },
      "source": [
        "## Carga de Ficheros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olS6ju_TyB8J"
      },
      "source": [
        "We import files from our personal Google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD3HOy94-mm2"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy9nIxF2-wbX"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read_csv(\n",
        "    \"PONER AQUI EL DATASET DE SENTIMIENTOS\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\",\n",
        "    encoding=\"latin1\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3PyqJhC-wTv"
      },
      "source": [
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU7HNoGyyD_q"
      },
      "source": [
        "## Pre Procesado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxfrD0KYyS0x"
      },
      "source": [
        "### Limpieza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V74EaTqS-wIL"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    # Removing the @\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
        "    # Removing the URL links\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
        "    # Keeping only letters\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
        "    # Removing additional whitespaces\n",
        "    tweet = re.sub(r\" +\", ' ', tweet)\n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5q9FcuQ-1Qk"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aJRlx5N-6-_"
      },
      "source": [
        "data_labels = data.sentiment.values\n",
        "data_labels[data_labels == 4] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6oFAPQCzmkj"
      },
      "source": [
        "### Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcSC8CRazrrn"
      },
      "source": [
        "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWMhfH9Q-1Ev"
      },
      "source": [
        "FullTokenizer = FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IYpAuZ3z3Ca"
      },
      "source": [
        "We only use the first sentence for BERT inputs so we add the CLS token at the beginning and the SEP token at the end of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPV8GvFr--Ze"
      },
      "source": [
        "def encode_sentence(sent):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m04GWhpZ--Vs"
      },
      "source": [
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dbz7MmSz7jq"
      },
      "source": [
        "We need to create the 3 different inputs for each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCVngAfF--ST"
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "    \n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH6PW-jZz9zP"
      },
      "source": [
        "### Creación del Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8CkvMeS0Bxe"
      },
      "source": [
        "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c7eSZ2u--Nj"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)]\n",
        "random.shuffle(data_with_len)\n",
        "data_with_len.sort(key=lambda x: x[2])\n",
        "sorted_all = [([get_ids(sent_lab[0]),\n",
        "                get_mask(sent_lab[0]),\n",
        "                get_segments(sent_lab[0])],\n",
        "               sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq8l7hvv--D3"
      },
      "source": [
        "# A list is a type of iterator so it can be used as generator for a dataset\n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kmvdTTW_Kht"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE,\n",
        "                                       padded_shapes=((3, None), ()),\n",
        "                                       padding_values=(0, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUbcPBso_Keu"
      },
      "source": [
        "NB_BATCHES = len(sorted_all) // BATCH_SIZE\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10\n",
        "NB_BATCHES_TRAIN = NB_BATCHES - NB_BATCHES_TEST\n",
        "all_batched.shuffle(NB_BATCHES)\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-_TD1-N0Fnw"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdvlHD07_Kb-"
      },
      "source": [
        "class BERTClassifier(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 name=\"bert_classifier\"):\n",
        "        super(BERTClassifier, self).__init__(name=name)\n",
        "\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        self.last_dense = layers.Dense(\n",
        "            units=nb_classes,\n",
        "            kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "    \n",
        "    def apply_bert(self, all_tokens):\n",
        "        pooled_output, _ = self.bert_layer([all_tokens[:, 0, :],\n",
        "                                            all_tokens[:, 1, :],\n",
        "                                            all_tokens[:, 2, :]])\n",
        "        \n",
        "        return pooled_output\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        output = self.apply_bert(inputs)\n",
        "        output = tf.nn.dropout(output, rate=self.dropout_rate)\n",
        "\n",
        "        probs = self.last_dense(output)\n",
        "\n",
        "        return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW188JZ50NSG"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBhijTTAlSz2"
      },
      "source": [
        "NB_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 5\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPBQk5zLqfxJ"
      },
      "source": [
        "bert_classifier = BERTClassifier(NB_CLASSES, DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPydSos7qnS6"
      },
      "source": [
        "NB_BATCHES_TRAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB8M9Ly5q-uo"
      },
      "source": [
        "train_dataset_light = train_dataset.take(500)\n",
        "steps_per_epoch_light = 100\n",
        "WARMUP_STEPS_LIGHT = int(500 * 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTgAYwWHrP2O"
      },
      "source": [
        "optimizer_light = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=500,\n",
        "    num_warmup_steps=WARMUP_STEPS_LIGHT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCZG2GoE-wSW"
      },
      "source": [
        "# very close but slightly better than standard categorical crossentropy loss\n",
        "def classification_loss_fn(labels, logits):\n",
        "    labels = tf.squeeze(labels)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "    one_hot_labels = tf.one_hot(\n",
        "        tf.cast(labels, dtype=tf.int32), depth=NB_CLASSES, dtype=tf.float32)\n",
        "    per_example_loss = -tf.reduce_sum(\n",
        "        tf.cast(one_hot_labels, dtype=tf.float32) * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9aOPV4rrlPj"
      },
      "source": [
        "bert_classifier.compile(optimizer_light,\n",
        "                        classification_loss_fn,\n",
        "                        [tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJIyGdrylSmk"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Model/BERT/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(bert_classifier=bert_classifier)\n",
        "\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fy8ne42bKWU"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvRvqrsqpoNI"
      },
      "source": [
        "bert_classifier.fit(train_dataset_light,\n",
        "                    steps_per_epoch=100,\n",
        "                    epochs=NB_EPOCHS,\n",
        "                    callbacks=[MyCustomCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF3qWz_90SVG"
      },
      "source": [
        "# Stage 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29bLIvwYVze4"
      },
      "source": [
        "bert_classifier.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXUyqwJAuVke"
      },
      "source": [
        "def get_prediction(sentence):\n",
        "    tokens = encode_sentence(sentence)\n",
        "\n",
        "    input_ids = get_ids(tokens)\n",
        "    input_mask = get_mask(tokens)\n",
        "    segment_ids = get_segments(tokens)\n",
        "\n",
        "    inputs = tf.stack(\n",
        "        [tf.cast(input_ids, dtype=tf.int32),\n",
        "         tf.cast(input_mask, dtype=tf.int32),\n",
        "         tf.cast(segment_ids, dtype=tf.int32)],\n",
        "         axis=0)\n",
        "    inputs = tf.expand_dims(inputs, 0) # simulates a batch\n",
        "\n",
        "    output = bert_classifier(inputs, training=False)\n",
        "\n",
        "    sentiment = tf.argmax(tf.squeeze(output)).numpy()\n",
        "\n",
        "    if sentiment == 0:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(\n",
        "            output))\n",
        "        return 0\n",
        "    elif sentiment == 1:\n",
        "        print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(\n",
        "            output))\n",
        "        return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnZuGmFSvuuZ"
      },
      "source": [
        "import tweepy\n",
        "\n",
        "api_key = \"\"\n",
        "api_secret_key = \"\n",
        "access_token = \"\"\n",
        "access_token_secret = \"\"\n",
        "\n",
        "# Create The Authenticate Object\n",
        "authenticate = tweepy.OAuthHandler(api_key, api_secret_key)\n",
        "\n",
        "# Set The Access Token & Access Token Secret\n",
        "authenticate.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# Create The API Object\n",
        "api = tweepy.API(authenticate, wait_on_rate_limit = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y49oQdtwwLmq"
      },
      "source": [
        "#Filters out mentions and RTs\n",
        "def from_creator(status):\n",
        "    if hasattr(status, 'retweeted_status'):\n",
        "        return False\n",
        "    elif status.in_reply_to_status_id != None:\n",
        "        return False\n",
        "    elif status.in_reply_to_screen_name != None:\n",
        "        return False\n",
        "    elif status.in_reply_to_user_id != None:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "class MyStreamListener(tweepy.StreamListener):\n",
        "\n",
        "    def on_status(self, status):\n",
        "        if from_creator(status):\n",
        "            tweet = status.text.lower()\n",
        "            value = get_prediction(tweet)\n",
        "            if \"doge\" in tweet:\n",
        "              if value:\n",
        "                order(TRADE_QUANTITY, SIDE_BUY, \"DOGEUSD\",order_type=ORDER_TYPE_MARKET)\n",
        "              else:\n",
        "                order(TRADE_QUANTITY, SIDE_SELL, \"DOGEUSD\",order_type=ORDER_TYPE_MARKET)\n",
        "            if \"bitcoin\" in tweet:\n",
        "              if value:\n",
        "                order(TRADE_QUANTITY, SIDE_BUY, \"BTCUSD\",order_type=ORDER_TYPE_MARKET)\n",
        "              else:\n",
        "                order(TRADE_QUANTITY, SIDE_SELL, \"BTCUSD\",order_type=ORDER_TYPE_MARKET)\n",
        "            return True\n",
        "        return True\n",
        "\n",
        "\n",
        "    def on_error(self, status_code):\n",
        "        if status_code == 420:\n",
        "            print(\"Error 420\")\n",
        "            #returning False in on_error disconnects the stream\n",
        "            return False\n",
        "    \n",
        "myStreamListener = MyStreamListener()\n",
        "myStream = tweepy.Stream(auth = api.auth, listener=myStreamListener)   \n",
        "myStream.filter(follow=['44196397'])             \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}